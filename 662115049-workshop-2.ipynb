{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b661e492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/win/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/win/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import kagglehub\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "textstat.set_lang('en_US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "191d6be0-62c5-46f2-9bc5-aee304a5f969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/win/dev/university/nlp/workshop/workshop-2/resource\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "destination_dir = current_dir + \"/resource\"\n",
    "\n",
    "if os.path.exists(destination_dir):\n",
    "    print(\"Path to dataset files:\", destination_dir)\n",
    "else:\n",
    "    source = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "    shutil.move(source, current_dir)\n",
    "    os.rename(current_dir + \"/1\", destination_dir)\n",
    "    print(\"Path to dataset files:\", destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ed356b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_fleash_ease(score):\n",
    "    if score <= 29:\n",
    "        return \"Confusing\"\n",
    "    elif score <= 49:\n",
    "        return \"Difficult\"\n",
    "    elif score <= 59:\n",
    "        return \"Fairly Difficult\"\n",
    "    elif score <= 69:\n",
    "        return \"Standard\"\n",
    "    elif score <= 79:\n",
    "        return \"Fairly Easy\"\n",
    "    elif score <= 89:\n",
    "        return \"Easy\"\n",
    "    elif score <= 100:\n",
    "        return \"Very Easy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fd5faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"resource/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "540a5c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.13603669316\n",
      "Standard\n"
     ]
    }
   ],
   "source": [
    "df['flesch-ease'] = df['review'].apply(textstat.flesch_reading_ease)\n",
    "\n",
    "score = df['flesch-ease'].mean()\n",
    "\n",
    "print(score)\n",
    "print(normalize_fleash_ease(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3467918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # replace <br /> with .\n",
    "    cleaned_text = text.replace('<br />', '. ')\n",
    "    # replace , with .\n",
    "    cleaned_text = cleaned_text.replace(',', '.')\n",
    "    # this regex will match everything that isn't letter, space or dot and remove them\n",
    "    # i keep dot becuase i need them to beable to tokenize it to sentence\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s.]', '', cleaned_text)\n",
    "    # this regex will match whitespace that is more than 1 character long and replace them with white space that is one character long\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    cleaned_text = cleaned_text.lower()\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "599ae2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_set = set(stop_words)\n",
    "    return \" \".join([word for word in text.split() if word not in stop_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a65996c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence):\n",
    "    words = nltk.word_tokenize(sentence);\n",
    "    filtered_words = [stemmer.stem(w) for w in words if w not in stop_words]\n",
    "    \n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bec3cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniza_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    cleaned_sentences = []\n",
    "    for s in sentences:\n",
    "        processed_sentence = process_sentence(s)\n",
    "        \n",
    "        # remove dot, and clean whitespace and remove trailing whitespace with .strip()\n",
    "        processed_sentence = processed_sentence.replace('.', '')\n",
    "        processed_sentence = re.sub(r'\\s+', ' ', processed_sentence).strip()\n",
    "        \n",
    "        # only add to output list if it not empty string\n",
    "        if processed_sentence:\n",
    "            cleaned_sentences.append(processed_sentence)\n",
    "\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79aa0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flesh_from_list(list):\n",
    "    scores = [textstat.flesch_reading_ease(s) for s in list]\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdd87035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.36963541967057\n",
      "Standard\n",
      "['one review mention watch oz episod youll hook', 'right', 'exactli happen', 'first thing struck oz brutal unflinch scene violenc', 'set right word go', 'trust', 'show faint heart timid', 'show pull punch regard drug', 'sex violenc', 'hardcor', 'classic use word', 'call oz nicknam given oswald maximum secur state penitentari', 'focus mainli emerald citi', 'experiment section prison cell glass front face inward', 'privaci high agenda', 'em citi home mani aryan', 'muslim', 'gangsta', 'latino', 'christian', 'italian', 'irish scuffl', 'death stare', 'dodgi deal shadi agreement never far away', 'would say main appeal show due fact goe show wouldnt dare', 'forget pretti pictur paint mainstream audienc', 'forget charm', 'forget romanc oz doesnt mess around', 'first episod ever saw struck nasti surreal', 'couldnt say readi', 'watch', 'develop tast oz', 'got accustom high level graphic violenc', 'violenc', 'injustic crook guard wholl sold nickel', 'inmat wholl kill order get away', 'well manner', 'middl class inmat turn prison bitch due lack street skill prison experi watch oz', 'may becom comfort uncomfort view that get touch darker side']\n"
     ]
    }
   ],
   "source": [
    "df['cleaned-data'] = df['review'].apply(clean_text).apply(tokeniza_text)\n",
    "df['flesch-ease'] = df['cleaned-data'].apply(get_flesh_from_list)\n",
    "\n",
    "score = df['flesch-ease'].mean()\n",
    "print(score)\n",
    "print(normalize_fleash_ease(score))\n",
    "print(df['cleaned-data'].iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
